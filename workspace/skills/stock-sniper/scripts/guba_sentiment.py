#!/usr/bin/env python3
"""
è‚¡å§èˆ†æƒ…è·å–å™¨ - è·å–ä¸œæ–¹è´¢å¯Œè‚¡å§æƒ…ç»ªæ•°æ®
"""

import json
import re
import ssl
import urllib.request
from datetime import datetime
from urllib.parse import quote

ssl._create_default_https_context = ssl._create_unverified_context

class GubaSentimentAnalyzer:
    """è‚¡å§èˆ†æƒ…åˆ†æå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Referer': 'https://guba.eastmoney.com/',
        }
    
    def fetch_guba_posts(self, stock_code, limit=50):
        """
        è·å–è‚¡å§å¸–å­
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            limit: è·å–å¸–å­æ•°é‡
        
        Returns:
            list: å¸–å­åˆ—è¡¨
        """
        posts = []
        
        try:
            # ä¸œæ–¹è´¢å¯Œè‚¡å§æ¥å£
            # code æ ¼å¼: 0å¼€å¤´æˆ–3å¼€å¤´ç”¨ 0.codeï¼Œ6å¼€å¤´ç”¨ 1.code
            if stock_code.startswith('6'):
                secid = f"1.{stock_code}"
            else:
                secid = f"0.{stock_code}"
            
            # è‚¡å§API
            url = f"https://guba.eastmoney.com/api/taobaolst?type=1&code={stock_code}&page=1"
            
            req = urllib.request.Request(url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                html = response.read().decode('utf-8', errors='ignore')
                posts = self._parse_guba_html(html, limit)
        except Exception as e:
            print(f"âš ï¸ è·å–è‚¡å§æ•°æ®å¤±è´¥: {e}")
        
        return posts
    
    def _parse_guba_html(self, html, limit):
        """è§£æè‚¡å§HTML"""
        posts = []
        
        try:
            # ç®€å•çš„æ­£åˆ™æå–ï¼ˆå®é™…éœ€è¦æ›´ç²¾ç¡®çš„è§£æï¼‰
            # æå–å¸–å­æ ‡é¢˜å’Œå†…å®¹
            title_pattern = r'class="l3.*?">\s*<a[^>]*>(.*?)</a>'
            titles = re.findall(title_pattern, html, re.DOTALL)
            
            for title in titles[:limit]:
                # æ¸…ç†HTMLæ ‡ç­¾
                clean_title = re.sub(r'<[^>]+>', '', title).strip()
                if clean_title:
                    posts.append({
                        'title': clean_title,
                        'sentiment': self._analyze_post_sentiment(clean_title)
                    })
        except Exception as e:
            print(f"âš ï¸ è§£æè‚¡å§æ•°æ®å¤±è´¥: {e}")
        
        return posts
    
    def _analyze_post_sentiment(self, text):
        """
        åˆ†æå•æ¡å¸–å­çš„æƒ…ç»ª
        
        Returns:
            'bullish', 'bearish', 'neutral'
        """
        bullish_words = ['æ¶¨åœ', 'å¤§æ¶¨', 'ä¹°å…¥', 'çœ‹å¥½', 'æ‹‰å‡', 'çªç ´', 'ç‰›è‚¡', 'èµšé’±', 'å†²', 'åƒè‚‰', 'æ¶¨åœ']
        bearish_words = ['è·Œåœ', 'å¤§è·Œ', 'å–å‡º', 'çœ‹ç©º', 'è·³æ°´', 'ç ´ä½', 'åƒåœ¾', 'äºé’±', 'è·‘', 'å‰²è‚‰', 'è·Œåœ']
        
        b_count = sum(1 for w in bullish_words if w in text)
        be_count = sum(1 for w in bearish_words if w in text)
        
        if b_count > be_count:
            return 'bullish'
        elif be_count > b_count:
            return 'bearish'
        else:
            return 'neutral'
    
    def analyze_sentiment(self, stock_code, limit=50):
        """
        åˆ†æè‚¡ç¥¨çš„æ•´ä½“èˆ†æƒ…
        
        Returns:
            dict: èˆ†æƒ…åˆ†æç»“æœ
        """
        posts = self.fetch_guba_posts(stock_code, limit)
        
        if not posts:
            return {
                'stock_code': stock_code,
                'total_posts': 0,
                'bullish': 0,
                'bearish': 0,
                'neutral': 0,
                'bullish_ratio': 0,
                'bearish_ratio': 0,
                'sentiment_score': 5.0,  # 0-10åˆ†
                'overall': 'ä¸­æ€§',
                'hot_keywords': [],
                'sample_posts': []
            }
        
        # ç»Ÿè®¡
        total = len(posts)
        bullish = sum(1 for p in posts if p['sentiment'] == 'bullish')
        bearish = sum(1 for p in posts if p['sentiment'] == 'bearish')
        neutral = total - bullish - bearish
        
        bullish_ratio = bullish / total if total > 0 else 0
        bearish_ratio = bearish / total if total > 0 else 0
        
        # æƒ…ç»ªå¾—åˆ† (0-10åˆ†)
        sentiment_score = 5 + (bullish_ratio - bearish_ratio) * 5
        sentiment_score = max(0, min(10, sentiment_score))
        
        # æ•´ä½“åˆ¤æ–­
        if sentiment_score >= 7:
            overall = 'åå¤š'
        elif sentiment_score <= 3:
            overall = 'åç©º'
        else:
            overall = 'ä¸­æ€§'
        
        # æå–çƒ­é—¨å…³é”®è¯
        hot_keywords = self._extract_keywords([p['title'] for p in posts])
        
        return {
            'stock_code': stock_code,
            'total_posts': total,
            'bullish': bullish,
            'bearish': bearish,
            'neutral': neutral,
            'bullish_ratio': round(bullish_ratio * 100, 1),
            'bearish_ratio': round(bearish_ratio * 100, 1),
            'sentiment_score': round(sentiment_score, 1),
            'overall': overall,
            'hot_keywords': hot_keywords[:10],
            'sample_posts': posts[:5]
        }
    
    def _extract_keywords(self, texts):
        """æå–çƒ­é—¨å…³é”®è¯"""
        # ç®€å•çš„è¯é¢‘ç»Ÿè®¡
        word_count = {}
        
        for text in texts:
            words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
            for word in words:
                if len(word) >= 2:
                    word_count[word] = word_count.get(word, 0) + 1
        
        # è¿‡æ»¤å¸¸è§è¯ï¼ŒæŒ‰é¢‘ç‡æ’åº
        stop_words = {'ä»Šæ—¥', 'æ€ä¹ˆ', 'ä»€ä¹ˆ', 'è¿™ä¸ª', 'ä¸€ä¸ª', 'å¤§å®¶', 'å¯ä»¥', 'ä»Šå¤©', 'æ˜å¤©', 'ç°åœ¨'}
        filtered = {k: v for k, v in word_count.items() if k not in stop_words and v >= 2}
        
        return sorted(filtered.keys(), key=lambda x: filtered[x], reverse=True)
    
    def get_hot_stocks_from_guba(self, limit=20):
        """
        è·å–è‚¡å§çƒ­é—¨è‚¡ç¥¨ï¼ˆæŒ‰è®¨è®ºçƒ­åº¦ï¼‰
        """
        hot_stocks = []
        
        try:
            url = "https://guba.eastmoney.com/rank/"
            req = urllib.request.Request(url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                html = response.read().decode('utf-8', errors='ignore')
                # è§£æçƒ­é—¨è‚¡ç¥¨åˆ—è¡¨
                # ç®€åŒ–å¤„ç†
        except Exception as e:
            print(f"âš ï¸ è·å–çƒ­é—¨è‚¡ç¥¨å¤±è´¥: {e}")
        
        return hot_stocks

def main():
    """æµ‹è¯•"""
    analyzer = GubaSentimentAnalyzer()
    
    # åˆ†ææŸåªè‚¡ç¥¨çš„èˆ†æƒ…
    stock_code = "000001"  # å¹³å®‰é“¶è¡Œ
    print(f"ğŸ“Š åˆ†æ {stock_code} è‚¡å§èˆ†æƒ…...\n")
    
    result = analyzer.analyze_sentiment(stock_code)
    
    print(f"æ€»å¸–æ•°: {result['total_posts']}")
    print(f"çœ‹å¤š: {result['bullish']} ({result['bullish_ratio']}%)")
    print(f"çœ‹ç©º: {result['bearish']} ({result['bearish_ratio']}%)")
    print(f"ä¸­æ€§: {result['neutral']}")
    print(f"\næƒ…ç»ªå¾—åˆ†: {result['sentiment_score']}/10")
    print(f"æ•´ä½“åˆ¤æ–­: {result['overall']}")
    
    if result['hot_keywords']:
        print(f"\nçƒ­é—¨å…³é”®è¯: {', '.join(result['hot_keywords'][:5])}")
    
    if result['sample_posts']:
        print("\nç¤ºä¾‹å¸–å­:")
        for i, post in enumerate(result['sample_posts'][:3], 1):
            emoji = 'ğŸŸ¢' if post['sentiment'] == 'bullish' else 'ğŸ”´' if post['sentiment'] == 'bearish' else 'âšª'
            print(f"  {i}. {emoji} {post['title'][:40]}...")

if __name__ == "__main__":
    main()
