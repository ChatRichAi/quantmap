#!/usr/bin/env python3
"""
ArXiv Meta-Extractor v2.0
æ¢æ–‡å³°é£æ ¼é‡æ„ç‰ˆ

æ ¸å¿ƒç†å¿µï¼š
1. ä¸ç›´æ¥äº¤æ˜“è®ºæ–‡å› å­ â†’ æå– META-PATTERNï¼ˆä»€ä¹ˆç»“æ„æœ‰æ•ˆï¼‰
2. è´Ÿé¢ç­›é€‰ â†’ è¯†åˆ«å¤±æ•ˆæ¨¡å¼ï¼Œå»ºç«‹é»‘åå•
3. å˜å¼‚å¼•æ“ â†’ å°†è®ºæ–‡é€»è¾‘æ‹†è§£ä¸ºå¯é‡ç»„çš„åŸºå› ç‰‡æ®µ

ä¸ v1 çš„åŒºåˆ«ï¼š
- v1: è®ºæ–‡ â†’ å› å­ â†’ åŸºå› æ± ï¼ˆç›´æ¥æ³¨å…¥ï¼Œé—®é¢˜ï¼šå‘è¡¨å³å¤±æ•ˆï¼‰
- v2: è®ºæ–‡ â†’ æ¨¡å¼æå– â†’ å˜å¼‚ç´ æåº“ â†’ äº¤å‰ç¹è¡ï¼ˆé—´æ¥ä»·å€¼ï¼‰
"""

import sys
import re
import json
import hashlib
import sqlite3
import requests
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from xml.etree import ElementTree as ET
from collections import defaultdict

sys.path.insert(0, '/Users/oneday/.openclaw/workspace/quantclaw')

from evolution_ecosystem import Gene


@dataclass 
class MetaPattern:
    """
    å…ƒæ¨¡å¼ - æè¿°"ä»€ä¹ˆæ ·çš„å› å­ç»“æ„å¯èƒ½æœ‰æ•ˆ"
    
    ä¸æ˜¯å…·ä½“å…¬å¼ï¼Œè€Œæ˜¯æŠ½è±¡æ¨¡æ¿ï¼š
    - "ä»·æ ¼çªç ´ + æˆäº¤é‡ç¡®è®¤" â†’ å¯å®ä¾‹åŒ–ä¸ºå¤šç§å…·ä½“å®ç°
    - "å‡å€¼å›å½’ + æ³¢åŠ¨ç‡è¿‡æ»¤" â†’ åŒä¸Š
    """
    pattern_id: str
    name: str
    category: str  # momentum, mean_reversion, etc.
    
    # ç»“æ„æè¿°
    logic_skeleton: str  # é€»è¾‘éª¨æ¶ï¼Œå¦‚ "CONDITION_A AND CONDITION_B"
    condition_a: Dict    # æ¡ä»¶Açš„æŠ½è±¡æè¿°
    condition_b: Dict    # æ¡ä»¶Bçš„æŠ½è±¡æè¿°
    
    # æ¥æºè¿½è¸ª
    source_papers: List[str]  # å“ªäº›è®ºæ–‡å‡ºç°æ­¤æ¨¡å¼
    occurrence_count: int     # å‡ºç°æ¬¡æ•°
    
    # æœ‰æ•ˆæ€§è¯„ä¼°
    fitness_estimate: float   # åŸºäºè®ºæ–‡å¼•ç”¨/å¹´ä»½çš„ä¼°è®¡
    decay_risk: float         # å¤±æ•ˆé£é™©ï¼ˆè€è®ºæ–‡æ›´é«˜ï¼‰
    
    # å¯å®ä¾‹åŒ–æ¨¡æ¿
    instantiation_templates: List[Dict]  # å…·ä½“å®ç°æ¨¡æ¿


@dataclass
class FailurePattern:
    """
    å¤±æ•ˆæ¨¡å¼ - è®°å½•å·²çŸ¥çš„å¤±è´¥å› å­ç»“æ„
    ç”¨äºè´Ÿé¢ç­›é€‰ï¼Œé¿å…é‡å¤è¸©å‘
    """
    pattern_id: str
    description: str
    failure_reason: str  # overfit, arbitrage_decay, data_mining
    source_evidence: List[str]  # è¯æ®æ¥æº
    confidence: float


class ArxivMetaMiner:
    """
    arXiv å…ƒæ¨¡å¼æŒ–æ˜å™¨
    
    æ ¸å¿ƒæ´å¯Ÿï¼š
    é‡åŒ–è®ºæ–‡çš„ä»·å€¼ä¸åœ¨äº"å…·ä½“å…¬å¼"ï¼Œè€Œåœ¨äº"ç»“æ„æœ‰æ•ˆæ€§"çš„ç»Ÿè®¡è¯æ®ã€‚
    å¦‚æœ 50 ç¯‡é«˜è´¨é‡è®ºæ–‡éƒ½åœ¨ç”¨"åŠ¨é‡+æ³¢åŠ¨ç‡è¿‡æ»¤"ï¼Œè¯´æ˜è¿™ä¸ªç»“æ„æœ‰ä»·å€¼ï¼Œ
    å³ä½¿å…·ä½“å‚æ•°å·²å¤±æ•ˆã€‚
    """
    
    # é€»è¾‘éª¨æ¶æ¨¡æ¿åº“
    SKELETON_PATTERNS = {
        'momentum_confirmation': {
            'skeleton': 'TREND_SIGNAL AND CONFIRMATION_SIGNAL',
            'description': 'è¶‹åŠ¿ä¿¡å·éœ€è¦ç¡®è®¤ä¿¡å·è¿‡æ»¤å‡çªç ´',
            'examples': [
                'price_breakout AND volume_surge',
                'ma_crossover AND rsi_strength',
                'new_high AND momentum_acceleration'
            ]
        },
        'mean_reversion_filter': {
            'skeleton': 'EXTREME_DEVIATION AND REVERSION_CATALYST',
            'description': 'æç«¯åç¦»+åè½¬å‚¬åŒ–å‰‚',
            'examples': [
                'zscore_extreme AND volume_climax',
                'rsi_oversold AND bullish_divergence',
                'bollinger_break AND mean_attraction'
            ]
        },
        'volatility_regime': {
            'skeleton': 'VOLATILITY_CONDITION AND DIRECTIONAL_BIAS',
            'description': 'æ³¢åŠ¨ç‡çŠ¶æ€ä¸‹çš„æ–¹å‘æ€§åå‘',
            'examples': [
                'low_vol_environment AND trend_following',
                'high_vol_spike AND mean_reversion',
                'vol_contraction AND breakout_setup'
            ]
        },
        'multi_timeframe': {
            'skeleton': 'HIGHER_TF_ALIGN AND LOWER_TF_ENTRY',
            'description': 'å¤šæ—¶é—´æ¡†æ¶å…±æŒ¯',
            'examples': [
                'weekly_uptrend AND daily_pullback',
                'monthly_breakout AND hourly_consolidation',
                'daily_support AND 15min_reversal'
            ]
        },
        'fundamental_technical': {
            'skeleton': 'FUNDAMENTAL_FILTER AND TECHNICAL_TRIGGER',
            'description': 'åŸºæœ¬é¢è¿‡æ»¤+æŠ€æœ¯é¢è§¦å‘',
            'examples': [
                'earnings_growth AND price_momentum',
                'value_cheap AND technical_breakout',
                'quality_high AND trend_following'
            ]
        }
    }
    
    # å…³é”®è¯åˆ°æ¡ä»¶çš„æ˜ å°„
    CONDITION_KEYWORDS = {
        'price_breakout': ['breakout', 'break through', 'exceeds', 'penetrates', 'surpasses'],
        'volume_surge': ['volume', 'turnover spike', 'increased volume', 'liquidity surge'],
        'ma_crossover': ['moving average', 'ma cross', 'golden cross', 'ma breakout'],
        'rsi_strength': ['rsi', 'relative strength', 'overbought', 'oversold'],
        'zscore_extreme': ['z-score', 'standard deviation', 'sigma', 'extreme deviation'],
        'volatility_low': ['low volatility', 'volatility compression', 'quiet period'],
        'volatility_high': ['high volatility', 'volatility expansion', 'volatile'],
        'trend_uptrend': ['uptrend', 'bullish trend', 'rising trend', 'positive trend'],
        'trend_downtrend': ['downtrend', 'bearish trend', 'falling trend', 'negative trend'],
        'support_level': ['support', 'demand zone', 'floor'],
        'resistance_level': ['resistance', 'supply zone', 'ceiling'],
    }
    
    def __init__(self, db_path: str = "evolution_hub.db"):
        self.db_path = db_path
        self.meta_patterns: Dict[str, MetaPattern] = {}
        self.failure_patterns: List[FailurePattern] = []
        self.paper_metadata: Dict[str, Dict] = {}
        
    def analyze_papers(self, papers: List[Dict]) -> Dict:
        """
        æ‰¹é‡åˆ†æè®ºæ–‡ï¼Œæå–å…ƒæ¨¡å¼
        """
        print("\n" + "="*70)
        print("ğŸ”¬ META-PATTERN EXTRACTION")
        print("="*70)
        
        for paper in papers:
            self._analyze_single_paper(paper)
        
        # èšåˆç»Ÿè®¡
        self._aggregate_patterns()
        
        # ç”ŸæˆæŠ¥å‘Š
        return self._generate_report()
    
    def _analyze_single_paper(self, paper: Dict):
        """åˆ†æå•ç¯‡è®ºæ–‡"""
        text = f"{paper['title']} {paper.get('summary', '')}".lower()
        arxiv_id = paper['id']
        
        # æå–å¹´ä»½ï¼ˆç”¨äº decay è¯„ä¼°ï¼‰
        year = self._extract_year(paper.get('published', ''))
        
        # 1. æ£€æµ‹é€»è¾‘éª¨æ¶åŒ¹é…
        matched_skeletons = self._detect_skeletons(text)
        
        # 2. æå–å…·ä½“æ¡ä»¶
        detected_conditions = self._detect_conditions(text)
        
        # 3. ç»„åˆæˆå…ƒæ¨¡å¼
        for skeleton_name in matched_skeletons:
            skeleton = self.SKELETON_PATTERNS[skeleton_name]
            
            # å°è¯•åŒ¹é…æ¡ä»¶
            condition_pairs = self._match_conditions_to_skeleton(
                skeleton, detected_conditions
            )
            
            for pair in condition_pairs:
                pattern_key = f"{skeleton_name}:{pair[0]}:{pair[1]}"
                
                if pattern_key not in self.meta_patterns:
                    self.meta_patterns[pattern_key] = MetaPattern(
                        pattern_id=hashlib.sha256(pattern_key.encode()).hexdigest()[:16],
                        name=f"{skeleton_name}_{pair[0]}_{pair[1]}",
                        category=self._categorize(skeleton_name),
                        logic_skeleton=skeleton['skeleton'],
                        condition_a={'type': pair[0], 'keywords': self.CONDITION_KEYWORDS.get(pair[0], [])},
                        condition_b={'type': pair[1], 'keywords': self.CONDITION_KEYWORDS.get(pair[1], [])},
                        source_papers=[arxiv_id],
                        occurrence_count=1,
                        fitness_estimate=self._estimate_fitness(year, paper),
                        decay_risk=self._calculate_decay_risk(year),
                        instantiation_templates=[]
                    )
                else:
                    pattern = self.meta_patterns[pattern_key]
                    pattern.occurrence_count += 1
                    pattern.source_papers.append(arxiv_id)
                    # æ›´æ–° fitnessï¼ˆå–å¹³å‡ï¼‰
                    pattern.fitness_estimate = (
                        pattern.fitness_estimate * (pattern.occurrence_count - 1) +
                        self._estimate_fitness(year, paper)
                    ) / pattern.occurrence_count
        
        # 4. æ£€æµ‹å¤±æ•ˆæ¨¡å¼ä¿¡å·
        self._detect_failure_signals(paper, text)
    
    def _detect_skeletons(self, text: str) -> List[str]:
        """æ£€æµ‹æ–‡æœ¬åŒ¹é…çš„é€»è¾‘éª¨æ¶"""
        matched = []
        
        # åŸºäºå…³é”®è¯ç»„åˆåŒ¹é…
        indicators = {
            'momentum_confirmation': ['momentum', 'trend', 'confirm', 'volume'],
            'mean_reversion_filter': ['reversion', 'mean', 'extreme', 'deviation'],
            'volatility_regime': ['volatility', 'regime', 'conditional', 'state'],
            'multi_timeframe': ['timeframe', 'frequency', 'daily', 'weekly', 'aggregate'],
            'fundamental_technical': ['fundamental', 'technical', 'filter', 'screen']
        }
        
        for skeleton, keywords in indicators.items():
            score = sum(1 for kw in keywords if kw in text)
            if score >= 2:  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
                matched.append(skeleton)
        
        return matched
    
    def _detect_conditions(self, text: str) -> Set[str]:
        """æ£€æµ‹æ–‡æœ¬ä¸­å‡ºç°çš„æ¡ä»¶ç±»å‹"""
        detected = set()
        
        for condition, keywords in self.CONDITION_KEYWORDS.items():
            for kw in keywords:
                if kw in text:
                    detected.add(condition)
                    break
        
        return detected
    
    def _match_conditions_to_skeleton(self, skeleton: Dict, conditions: Set[str]) -> List[Tuple[str, str]]:
        """å°†æ£€æµ‹åˆ°çš„æ¡ä»¶åŒ¹é…åˆ°éª¨æ¶çš„å ä½ç¬¦"""
        pairs = []
        
        # ç®€åŒ–ç­–ç•¥ï¼šéšæœºç»„åˆä¸¤ä¸ªæ£€æµ‹åˆ°çš„æ¡ä»¶
        # å®é™…åº”è¯¥ç”¨æ›´æ™ºèƒ½çš„è¯­ä¹‰åŒ¹é…
        conditions_list = list(conditions)
        
        if len(conditions_list) >= 2:
            for i in range(min(len(conditions_list), 3)):
                for j in range(i+1, min(len(conditions_list), 4)):
                    pairs.append((conditions_list[i], conditions_list[j]))
        
        return pairs
    
    def _extract_year(self, published: str) -> int:
        """ä»æ—¥æœŸå­—ç¬¦ä¸²æå–å¹´ä»½"""
        try:
            return int(published[:4])
        except:
            return datetime.now().year
    
    def _estimate_fitness(self, year: int, paper: Dict) -> float:
        """
        åŸºäºè®ºæ–‡å…ƒæ•°æ®ä¼°è®¡å› å­æœ‰æ•ˆæ€§
        
        å¯å‘å¼è§„åˆ™ï¼š
        - æ–°è®ºæ–‡ï¼ˆ2023+ï¼‰ï¼š0.6ï¼ˆå¯èƒ½æœ‰ç”Ÿå­˜åå·®ï¼‰
        - ç»å…¸è®ºæ–‡ï¼ˆ2015-2022ï¼‰ï¼š0.7ï¼ˆç»è¿‡æ—¶é—´è€ƒéªŒï¼‰
        - è€è®ºæ–‡ï¼ˆ<2015ï¼‰ï¼š0.4ï¼ˆå¯èƒ½å·²å¤±æ•ˆï¼‰
        """
        current_year = datetime.now().year
        age = current_year - year
        
        if age <= 2:
            return 0.6  # æ–°è®ºæ–‡ï¼Œå¯èƒ½æœ‰å‘è¡¨åè¯¯
        elif age <= 8:
            return 0.7  # é»„é‡‘æœŸ
        else:
            return max(0.3, 0.7 - (age - 8) * 0.05)  # çº¿æ€§è¡°å‡
    
    def _calculate_decay_risk(self, year: int) -> float:
        """è®¡ç®—å¤±æ•ˆé£é™©"""
        age = datetime.now().year - year
        return min(0.9, age * 0.08)  # æ¯å¹´8%é£é™©ç´¯ç§¯
    
    def _categorize(self, skeleton_name: str) -> str:
        """åˆ†ç±»"""
        mapping = {
            'momentum_confirmation': 'momentum',
            'mean_reversion_filter': 'mean_reversion',
            'volatility_regime': 'volatility',
            'multi_timeframe': 'multi_tf',
            'fundamental_technical': 'hybrid'
        }
        return mapping.get(skeleton_name, 'unknown')
    
    def _detect_failure_signals(self, paper: Dict, text: str):
        """æ£€æµ‹è®ºæ–‡ä¸­çš„å¤±æ•ˆä¿¡å·"""
        failure_signals = [
            ('data mining', 'data_mining'),
            ('overfitting', 'overfit'),
            ('in-sample', 'overfit'),
            ('survivorship bias', 'bias'),
            ('transaction costs', 'friction'),
            ('no longer profitable', 'decay'),
            ('disappeared', 'decay'),
        ]
        
        for signal, reason in failure_signals:
            if signal in text:
                self.failure_patterns.append(FailurePattern(
                    pattern_id=hashlib.sha256(f"{paper['id']}:{signal}".encode()).hexdigest()[:16],
                    description=f"Detected in {paper['title'][:50]}",
                    failure_reason=reason,
                    source_evidence=[paper['id']],
                    confidence=0.6
                ))
    
    def _aggregate_patterns(self):
        """èšåˆç»Ÿè®¡ï¼Œè¯†åˆ«é«˜é¢‘æ¨¡å¼"""
        print(f"\nğŸ“Š Pattern Aggregation:")
        print(f"   Total unique patterns: {len(self.meta_patterns)}")
        print(f"   Failure signals detected: {len(self.failure_patterns)}")
        
        # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
        sorted_patterns = sorted(
            self.meta_patterns.values(),
            key=lambda p: p.occurrence_count,
            reverse=True
        )
        
        print(f"\n   Top patterns by frequency:")
        for p in sorted_patterns[:5]:
            print(f"   - {p.name}: {p.occurrence_count} papers, fitness={p.fitness_estimate:.2f}")
    
    def _generate_report(self) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        return {
            'patterns': [asdict(p) for p in self.meta_patterns.values()],
            'failures': [asdict(f) for f in self.failure_patterns],
            'stats': {
                'total_patterns': len(self.meta_patterns),
                'high_confidence_patterns': len([p for p in self.meta_patterns.values() if p.occurrence_count >= 3]),
                'failure_signals': len(self.failure_patterns)
            }
        }
    
    def save_to_db(self):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºå…ƒæ¨¡å¼è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS meta_patterns (
                pattern_id TEXT PRIMARY KEY,
                name TEXT,
                category TEXT,
                logic_skeleton TEXT,
                condition_a TEXT,
                condition_b TEXT,
                source_papers TEXT,
                occurrence_count INTEGER,
                fitness_estimate REAL,
                decay_risk REAL,
                created_at TEXT
            )
        ''')
        
        # åˆ›å»ºå¤±æ•ˆæ¨¡å¼è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS failure_patterns (
                pattern_id TEXT PRIMARY KEY,
                description TEXT,
                failure_reason TEXT,
                source_evidence TEXT,
                confidence REAL,
                created_at TEXT
            )
        ''')
        
        # æ’å…¥å…ƒæ¨¡å¼
        for pattern in self.meta_patterns.values():
            cursor.execute('''
                INSERT OR REPLACE INTO meta_patterns VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                pattern.pattern_id,
                pattern.name,
                pattern.category,
                pattern.logic_skeleton,
                json.dumps(pattern.condition_a),
                json.dumps(pattern.condition_b),
                json.dumps(pattern.source_papers),
                pattern.occurrence_count,
                pattern.fitness_estimate,
                pattern.decay_risk,
                datetime.now().isoformat()
            ))
        
        conn.commit()
        conn.close()
        print(f"\nğŸ’¾ Saved {len(self.meta_patterns)} patterns to database")


class MetaPatternInstantiator:
    """
    å…ƒæ¨¡å¼å®ä¾‹åŒ–å™¨
    
    å°†æŠ½è±¡çš„å…ƒæ¨¡å¼è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„å…·ä½“åŸºå› 
    ä¾‹å¦‚ï¼š"ä»·æ ¼çªç ´ + æˆäº¤é‡ç¡®è®¤" â†’ å…·ä½“çš„ `close > max(high[-20:]) AND volume > mean(volume[-20:]) * 1.5`
    """
    
    # æ¡ä»¶åˆ°ä»£ç çš„æ˜ å°„
    CONDITION_TEMPLATES = {
        'price_breakout': {
            'python': 'close > max(high[-{period}:])',
            'params': {'period': [10, 20, 60]}
        },
        'volume_surge': {
            'python': 'volume > mean(volume[-{period}:]) * {mult}',
            'params': {'period': [20], 'mult': [1.5, 2.0, 3.0]}
        },
        'rsi_strength': {
            'python': 'RSI(close, {period}) {op} {threshold}',
            'params': {'period': [14], 'op': ['>', '<'], 'threshold': [30, 70]}
        },
        'ma_crossover': {
            'python': 'MA(close, {fast}) {op} MA(close, {slow})',
            'params': {'fast': [5, 10, 20], 'slow': [20, 60, 120], 'op': ['>', '<']}
        },
        'zscore_extreme': {
            'python': 'abs((close - mean(close[-{period}:])) / std(close[-{period}:])) > {threshold}',
            'params': {'period': [20, 60], 'threshold': [2.0, 2.5, 3.0]}
        },
        'volatility_low': {
            'python': 'ATR(close, {period}) / close < {threshold}',
            'params': {'period': [14], 'threshold': [0.02, 0.03, 0.05]}
        },
        'trend_uptrend': {
            'python': 'close > MA(close, {period})',
            'params': {'period': [20, 60, 120]}
        },
        'support_level': {
            'python': 'close >= min(low[-{period}:]) * (1 + {tolerance})',
            'params': {'period': [20, 60], 'tolerance': [0.0, 0.02]}
        }
    }
    
    def __init__(self, db_path: str = "evolution_hub.db"):
        self.db_path = db_path
    
    def instantiate(self, meta_pattern: MetaPattern, max_instances: int = 5) -> List[Gene]:
        """
        å°†å…ƒæ¨¡å¼å®ä¾‹åŒ–ä¸ºå…·ä½“åŸºå› 
        
        ä½¿ç”¨ç¬›å¡å°”ç§¯ç”Ÿæˆå‚æ•°ç»„åˆï¼Œä½†é™åˆ¶æ•°é‡é¿å…çˆ†ç‚¸
        """
        genes = []
        
        cond_a = meta_pattern.condition_a.get('type')
        cond_b = meta_pattern.condition_b.get('type')
        
        template_a = self.CONDITION_TEMPLATES.get(cond_a)
        template_b = self.CONDITION_TEMPLATES.get(cond_b)
        
        if not template_a or not template_b:
            return genes
        
        # ç”Ÿæˆå‚æ•°ç»„åˆï¼ˆé™åˆ¶æ•°é‡ï¼‰
        import itertools
        
        params_a_list = self._generate_param_combinations(template_a['params'])
        params_b_list = self._generate_param_combinations(template_b['params'])
        
        count = 0
        for pa in params_a_list[:3]:  # é™åˆ¶æ¯æ¡ä»¶3ç§
            for pb in params_b_list[:3]:
                if count >= max_instances:
                    break
                
                # æ„å»ºå…¬å¼
                formula_a = template_a['python'].format(**pa)
                formula_b = template_b['python'].format(**pb)
                full_formula = f"({formula_a}) AND ({formula_b})"
                
                # åˆ›å»ºåŸºå› 
                gene = Gene(
                    gene_id=hashlib.sha256(full_formula.encode()).hexdigest()[:16],
                    name=f"META_{meta_pattern.name}_{count}",
                    description=f"Instantiated from {meta_pattern.name}",
                    formula=full_formula,
                    parameters={**pa, **pb, 'meta_pattern_id': meta_pattern.pattern_id},
                    source=f"meta_pattern:{meta_pattern.pattern_id}",
                    author="MetaInstantiator",
                    created_at=datetime.now(),
                    generation=1
                )
                
                genes.append(gene)
                count += 1
        
        return genes
    
    def _generate_param_combinations(self, params: Dict) -> List[Dict]:
        """ç”Ÿæˆå‚æ•°ç»„åˆ"""
        keys = list(params.keys())
        values = [params[k] if isinstance(params[k], list) else [params[k]] for k in keys]
        
        import itertools
        combinations = []
        for combo in itertools.product(*values):
            combinations.append(dict(zip(keys, combo)))
        
        return combinations
    
    def batch_instantiate(self, min_occurrence: int = 3, dry_run: bool = False) -> List[Gene]:
        """
        æ‰¹é‡å®ä¾‹åŒ–é«˜é¢‘å…ƒæ¨¡å¼
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–é«˜é¢‘æ¨¡å¼
        cursor.execute('''
            SELECT * FROM meta_patterns 
            WHERE occurrence_count >= ? 
            ORDER BY occurrence_count DESC
        ''', (min_occurrence,))
        
        rows = cursor.fetchall()
        conn.close()
        
        all_genes = []
        
        print(f"\nğŸ§¬ Instantiating {len(rows)} high-frequency patterns...")
        
        for row in rows:
            pattern = MetaPattern(
                pattern_id=row[0],
                name=row[1],
                category=row[2],
                logic_skeleton=row[3],
                condition_a=json.loads(row[4]),
                condition_b=json.loads(row[5]),
                source_papers=json.loads(row[6]),
                occurrence_count=row[7],
                fitness_estimate=row[8],
                decay_risk=row[9],
                instantiation_templates=[]
            )
            
            genes = self.instantiate(pattern, max_instances=3)
            all_genes.extend(genes)
            
            print(f"   {pattern.name}: {len(genes)} instances")
        
        if not dry_run:
            self._save_genes(all_genes)
        
        return all_genes
    
    def _save_genes(self, genes: List[Gene]):
        """ä¿å­˜åŸºå› åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        inserted = 0
        for gene in genes:
            try:
                cursor.execute('''
                    INSERT OR IGNORE INTO genes VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    gene.gene_id, gene.name, gene.description, gene.formula,
                    json.dumps(gene.parameters), gene.source, gene.author,
                    gene.created_at.isoformat(), gene.parent_gene_id, gene.generation
                ))
                if cursor.rowcount > 0:
                    inserted += 1
            except:
                pass
        
        conn.commit()
        conn.close()
        print(f"\nğŸ’¾ Saved {inserted}/{len(genes)} instantiated genes")


# å‘½ä»¤è¡Œæ¥å£
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ArXiv Meta-Extractor v2.0')
    parser.add_argument('--fetch', '-f', action='store_true', help='Fetch papers from arXiv')
    parser.add_argument('--search', '-s', default='factor investing momentum', help='Search query')
    parser.add_argument('--limit', '-l', type=int, default=50, help='Paper limit')
    parser.add_argument('--instantiate', '-i', action='store_true', help='Instantiate meta patterns')
    parser.add_argument('--min-occurrence', '-m', type=int, default=3, help='Min occurrence for instantiation')
    parser.add_argument('--db', default='evolution_hub.db', help='Database path')
    
    args = parser.parse_args()
    
    if args.fetch:
        # è·å–è®ºæ–‡
        print("Fetching papers from arXiv...")
        # è¿™é‡Œå¤ç”¨ v1 çš„ API å®¢æˆ·ç«¯
        sys.path.insert(0, '/Users/oneday/.openclaw/workspace/quantclaw')
        from arxiv_gene_extractor import ArXivAPI
        
        api = ArXivAPI()
        papers = api.search(args.search, max_results=args.limit)
        
        # åˆ†æ
        miner = ArxivMetaMiner(args.db)
        report = miner.analyze_papers(papers)
        miner.save_to_db()
        
        print(f"\nâœ… Analysis complete: {report['stats']['total_patterns']} patterns extracted")
    
    if args.instantiate:
        # å®ä¾‹åŒ–
        instantiator = MetaPatternInstantiator(args.db)
        genes = instantiator.batch_instantiate(min_occurrence=args.min_occurrence)
        print(f"\nâœ… Instantiation complete: {len(genes)} genes created")
