#!/usr/bin/env python3
"""
QuantClaw RL Parameter Optimizer
å¼ºåŒ–å­¦ä¹ å‚æ•°ä¼˜åŒ–å™¨ - è‡ªåŠ¨ä¼˜åŒ–æœ€ä½³å› å­å‚æ•°

é’ˆå¯¹ Hurst Exponent (è¡¨ç°æœ€ä½³: å¤æ™®0.88, æ”¶ç›Š28.3%) è¿›è¡Œä¸“é¡¹ä¼˜åŒ–
"""

import sys
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple
import random

sys.path.insert(0, '/Users/oneday/.openclaw/workspace/quantclaw')

from factor_backtest_validator import FactorValidator, BacktestEngine
from evolution_ecosystem import QuantClawEvolutionHub, Gene


class RLParameterOptimizer:
    """
    å¼ºåŒ–å­¦ä¹ å‚æ•°ä¼˜åŒ–å™¨
    
    ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¼˜åŒ–å› å­å‚æ•°
    """
    
    def __init__(self, base_gene: Gene):
        self.base_gene = base_gene
        self.validator = FactorValidator()
        
        # å‚æ•°ç©ºé—´
        self.param_space = {
            'period': [50, 100, 150, 200],
            'threshold_long': [0.55, 0.6, 0.65, 0.7],
            'threshold_short': [0.3, 0.35, 0.4, 0.45],
            'stop_loss': [0.03, 0.05, 0.07, 0.10]
        }
        
        # ç­–ç•¥ç½‘ç»œ (ç®€åŒ–ç‰ˆï¼šå‚æ•°æ¦‚ç‡åˆ†å¸ƒ)
        self.policy = {k: {v: 1.0/len(vs) for v in vs} 
                       for k, vs in self.param_space.items()}
        
        # å­¦ä¹ ç‡
        self.lr = 0.1
        
    def sample_parameters(self) -> Dict:
        """æ ¹æ®å½“å‰ç­–ç•¥é‡‡æ ·å‚æ•°"""
        params = {}
        for param_name, probs in self.policy.items():
            values = list(probs.keys())
            probabilities = list(probs.values())
            params[param_name] = np.random.choice(values, p=probabilities)
        return params
    
    def create_variant_gene(self, params: Dict) -> Gene:
        """åˆ›å»ºå‚æ•°å˜ä½“åŸºå› """
        variant_id = f"{self.base_gene.gene_id}_rl_{hash(str(params)) % 10000}"
        
        # æ ¹æ®å‚æ•°ä¿®æ”¹å…¬å¼
        formula = f"Hurst(period={params['period']}) > {params['threshold_long']}"
        
        return Gene(
            gene_id=variant_id,
            name=f"{self.base_gene.name}_RL",
            description=f"RL optimized Hurst: period={params['period']}, "
                       f"long={params['threshold_long']}, "
                       f"short={params['threshold_short']}, "
                       f"SL={params['stop_loss']}",
            formula=formula,
            parameters=params,
            source=f"rl_optimization:{self.base_gene.gene_id}",
            author="rl_optimizer",
            created_at=datetime.now(),
            parent_gene_id=self.base_gene.gene_id,
            generation=self.base_gene.generation + 1
        )
    
    def evaluate_variant(self, gene: Gene, symbol: str = 'AAPL') -> Tuple[float, Dict]:
        """è¯„ä¼°å‚æ•°å˜ä½“"""
        results = self.validator.validate_gene(gene, symbols=[symbol])
        
        if not results:
            return -100, {}
        
        result = results[0]
        
        # è®¡ç®—å¥–åŠ± (å¤šç›®æ ‡)
        reward = (
            result.sharpe_ratio * 30 +           # å¤æ™®æƒé‡
            (1 - abs(result.max_drawdown) / 0.5) * 25 +  # å›æ’¤æ§åˆ¶
            result.win_rate * 20 +                # èƒœç‡
            max(result.annual_return, 0) / 0.5 * 25   # æ”¶ç›Š
        )
        
        metrics = {
            'sharpe': result.sharpe_ratio,
            'drawdown': result.max_drawdown,
            'return': result.annual_return,
            'win_rate': result.win_rate
        }
        
        return reward, metrics
    
    def update_policy(self, params: Dict, reward: float):
        """æ ¹æ®å¥–åŠ±æ›´æ–°ç­–ç•¥"""
        # ç®€åŒ–ç‰ˆç­–ç•¥æ¢¯åº¦ï¼šå¢åŠ é«˜å¥–åŠ±å‚æ•°çš„æ¦‚ç‡
        for param_name, param_value in params.items():
            if reward > 0:
                # å¢åŠ è¯¥å‚æ•°çš„æ¦‚ç‡
                self.policy[param_name][param_value] += self.lr * reward / 100
                
                # å½’ä¸€åŒ–
                total = sum(self.policy[param_name].values())
                for k in self.policy[param_name]:
                    self.policy[param_name][k] /= total
    
    def optimize(self, iterations: int = 20) -> List[Tuple[Gene, float, Dict]]:
        """
        è¿è¡Œä¼˜åŒ–
        
        Args:
            iterations: ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
            
        Returns:
            ä¼˜åŒ–åçš„åŸºå› åˆ—è¡¨ [(gene, reward, metrics), ...]
        """
        print("=" * 70)
        print("ğŸš€ RL Parameter Optimizer")
        print(f"   Target: {self.base_gene.name}")
        print(f"   Iterations: {iterations}")
        print("=" * 70)
        
        self.validator.connect()
        
        best_results = []
        
        try:
            for i in range(iterations):
                print(f"\nğŸ“Š Iteration {i+1}/{iterations}")
                
                # é‡‡æ ·å‚æ•°
                params = self.sample_parameters()
                print(f"   Params: {params}")
                
                # åˆ›å»ºå˜ä½“
                variant = self.create_variant_gene(params)
                
                # è¯„ä¼°
                reward, metrics = self.evaluate_variant(variant)
                print(f"   Reward: {reward:.2f} | Sharpe: {metrics.get('sharpe', 0):.2f}")
                
                # ä¿å­˜ç»“æœ
                best_results.append((variant, reward, metrics))
                
                # æ›´æ–°ç­–ç•¥
                self.update_policy(params, reward)
                
                # å¦‚æœè¡¨ç°ä¼˜ç§€ï¼Œä¿å­˜åˆ°åŸºå› æ± 
                if reward > 60:
                    hub = QuantClawEvolutionHub()
                    hub.publish_gene(variant)
                    print(f"   âœ… Saved to gene pool!")
            
        finally:
            self.validator.disconnect()
        
        # æ’åºå¹¶è¿”å›æœ€ä½³ç»“æœ
        best_results.sort(key=lambda x: x[1], reverse=True)
        
        print("\n" + "=" * 70)
        print("ğŸ‰ Optimization Complete!")
        print("=" * 70)
        
        # æ˜¾ç¤ºTop 5
        print("\nğŸ† Top 5 Variants:")
        for i, (gene, reward, metrics) in enumerate(best_results[:5], 1):
            print(f"{i}. {gene.name}")
            print(f"   Reward: {reward:.2f}")
            print(f"   Sharpe: {metrics.get('sharpe', 0):.2f}")
            print(f"   Return: {metrics.get('return', 0):.1%}")
            print(f"   MaxDD: {metrics.get('drawdown', 0):.1%}")
            print()
        
        return best_results


def main():
    """ä¸»å‡½æ•° - ä¼˜åŒ–Hurst Exponent"""
    
    # åˆ›å»ºåŸºç¡€HurståŸºå› 
    base_hurst = Gene(
        gene_id='g_hurst_base',
        name='Hurst Exponent Base',
        description='Trend persistence measure',
        formula='Hurst(close, 100)',
        parameters={'period': 100},
        source='optimization_target',
        author='system',
        created_at=datetime.now()
    )
    
    # è¿è¡Œä¼˜åŒ–
    optimizer = RLParameterOptimizer(base_hurst)
    results = optimizer.optimize(iterations=15)
    
    # ä¿å­˜æœ€ä½³ç»“æœåˆ°æ–‡ä»¶
    with open('rl_optimization_results.json', 'w') as f:
        json.dump([{
            'gene_id': g.gene_id,
            'name': g.name,
            'formula': g.formula,
            'parameters': g.parameters,
            'reward': r,
            'metrics': m
        } for g, r, m in results[:10]], f, indent=2, default=str)
    
    print(f"\nğŸ’¾ Results saved to: rl_optimization_results.json")


if __name__ == "__main__":
    main()
